{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ruru22/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ruru22/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ruru22/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn as sk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import datetime\n",
    "import re\n",
    "import praw\n",
    "import jsonlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_limits = ['2019-03-01 00:00:00', \n",
    "               '2019-06-01 00:00:00', \n",
    "               '2019-09-01 00:00:00', \n",
    "               '2020-01-01 00:00:00', \n",
    "               '2020-03-01 00:00:00', \n",
    "               '2020-06-01 00:00:00', \n",
    "               '2020-09-01 00:00:00', \n",
    "               '2021-01-01 00:00:00', \n",
    "               '2021-03-01 00:00:00', \n",
    "               '2021-06-01 00:00:00'\n",
    "            ]\n",
    "\n",
    "COUNT = len(date_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    created  \\\n",
      "360671  2018-12-31 02:25:14   \n",
      "270512  2018-12-31 02:27:21   \n",
      "112640  2018-12-31 02:27:56   \n",
      "294115  2018-12-31 02:29:05   \n",
      "214742  2018-12-31 02:29:18   \n",
      "...                     ...   \n",
      "498320  2021-04-09 21:26:18   \n",
      "427024  2021-04-09 21:26:19   \n",
      "816977  2021-04-09 21:26:20   \n",
      "144817  2021-04-09 21:26:21   \n",
      "516521  2021-04-09 21:26:28   \n",
      "\n",
      "                                                    title      id  \n",
      "360671  پسکوف: هیچ اقدامی علیه مظنونان معرفی گردیده در...  ab2og2  \n",
      "270512  One of the companies contracted by the governm...  ab2p2z  \n",
      "112640  Donald Trump: Outgoing chief of staff John Kel...  ab2p8u  \n",
      "294115  Todd Bowles Fired as Jets Head Coach After 4 S...  ab2pl2  \n",
      "214742  Macron's former aide admits using diplomatic p...  ab2pn3  \n",
      "...                                                   ...     ...  \n",
      "498320  Prince Charles Visits Queen Elizabeth II After...  mnr2wp  \n",
      "427024  Job postings hint at winners of NYC and London...  mnr2x1  \n",
      "816977  Tell Us The Most Underrated Moment From A Rom-...  mnr2xi  \n",
      "144817  15 Things From Lelo With Such Impressive Revie...  mnr2xw  \n",
      "516521  House Ethics Committee Investigating Florida G...  mnr30v  \n",
      "\n",
      "[818990 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def dfFromCSV(filename):\n",
    "    posts = pd.read_csv(filename)\n",
    "    return posts\n",
    "\n",
    "def dfToCSV(posts, filename):\n",
    "    posts.to_csv(filename, index=False)\n",
    "\n",
    "def postsFromNDJSON(filename):\n",
    "    posts = []\n",
    "    with jsonlines.open(filename) as reader:\n",
    "        for line in reader:\n",
    "            line = line[\"Item\"]\n",
    "            posts.append([line[\"created\"][\"S\"], line[\"title\"][\"S\"], line[\"id\"][\"S\"]])\n",
    "    posts = pd.DataFrame(posts,columns=['created', 'title', 'id'])\n",
    "\n",
    "    return posts\n",
    "\n",
    "def sortByTime(data):\n",
    "    data = data.sort_values(by='created')\n",
    "    return data\n",
    "\n",
    "posts = postsFromNDJSON(\"worldnews.jsonl\")\n",
    "posts = sortByTime(posts)\n",
    "posts.drop(posts.loc[posts['title'] == 'None'].index, inplace=True)\n",
    "dfToCSV(posts=posts, filename=\"worldnews.csv\")\n",
    "print(posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "50.0\n",
      "100.0\n",
      "150.0\n",
      "200.0\n",
      "250.0\n",
      "300.0\n",
      "350.0\n",
      "400.0\n",
      "450.0\n",
      "500.0\n",
      "550.0\n",
      "600.0\n",
      "650.0\n",
      "700.0\n",
      "750.0\n",
      "800.0\n",
      "Took 0:01:19.261132 seconds\n",
      "                    created  \\\n",
      "1       2018-12-31 02:27:21   \n",
      "2       2018-12-31 02:27:56   \n",
      "3       2018-12-31 02:29:05   \n",
      "4       2018-12-31 02:29:18   \n",
      "5       2018-12-31 02:30:30   \n",
      "...                     ...   \n",
      "818985  2021-04-09 21:26:18   \n",
      "818986  2021-04-09 21:26:19   \n",
      "818987  2021-04-09 21:26:20   \n",
      "818988  2021-04-09 21:26:21   \n",
      "818989  2021-04-09 21:26:28   \n",
      "\n",
      "                                                    title      id  \n",
      "1       one company contracted government charter ferr...  ab2p2z  \n",
      "2       donald trump outgoing chief staff john kelly w...  ab2p8u  \n",
      "3        todd bowles fired jet head coach  4  season team  ab2pl2  \n",
      "4       macron former aide admits using diplomatic pas...  ab2pn3  \n",
      "5       pep guardiola liverpool might best team world ...  ab2pzl  \n",
      "...                                                   ...     ...  \n",
      "818985  prince charles visit queen elizabeth ii prince...  mnr2wp  \n",
      "818986           job posting hint winner nyc london pilot  mnr2x1  \n",
      "818987                     tell u underrated moment movie  mnr2xi  \n",
      "818988   15  thing lelo impressive review may feel ins...  mnr2xw  \n",
      "818989  house ethic committee investigating florida go...  mnr30v  \n",
      "\n",
      "[743052 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove links\n",
    "    words = []\n",
    "    for token in tokens: \n",
    "        if 'http' not in token and 'www' not in token:\n",
    "            words.append(token)\n",
    "\n",
    "    # Remove stopwords and lemmatize words\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    words_split = []\n",
    "    for word in words:\n",
    "        words_split.extend(re.split('(\\d+)',word))\n",
    "\n",
    "    return ' '.join(words_split)\n",
    "\n",
    "def isEnglish(text):\n",
    "    try:\n",
    "        text.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def separateNumbersAlphabets(str):\n",
    "    numbers = re.findall(r'[0-9]+', str)\n",
    "    alphabets = re.findall(r'[a-zA-Z]+', str)\n",
    "    print(*numbers)\n",
    "    print(*alphabets)\n",
    "\n",
    "posts = dfFromCSV('worldnews.csv')\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lemmed = []\n",
    "\n",
    "for idx in posts.index:\n",
    "    if idx % 50000 == 0:\n",
    "        print(idx / 1000)\n",
    "    text = posts['title'][idx]\n",
    "\n",
    "    text = preprocess(text)\n",
    "\n",
    "    if isEnglish(text):\n",
    "        lemmed.append(text)\n",
    "    else:\n",
    "        lemmed.append('DELETEME')\n",
    "\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Took\", stop - start, \"seconds\")\n",
    "\n",
    "posts[\"title\"] = lemmed\n",
    "\n",
    "posts.drop(posts.loc[posts['title'] == 'DELETEME'].index, inplace=True)\n",
    "posts.drop(posts.loc[posts['title'] == ''].index, inplace=True)\n",
    "\n",
    "print(posts)\n",
    "dfToCSV(posts, 'lemmatized_ascii.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
